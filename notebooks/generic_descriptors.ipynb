{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "def extract_image_features(image):\n",
    "    global index\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    image_bgr = cv2.cvtColor(image.T, cv2.COLOR_RGB2BGR)\n",
    "    _ , descriptors =  sift.detectAndCompute(image_bgr, None)\n",
    "    try:\n",
    "        descriptors.shape\n",
    "    except AttributeError:\n",
    "        print(index)\n",
    "        index = index + 1\n",
    "        return np.empty([0, 128])\n",
    "    index = index + 1\n",
    "    return descriptors\n",
    "\n",
    "def process_images(images):\n",
    "    return np.concatenate([extract_image_features(image) for image in images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fisher_vector(xx, gmm):\n",
    "    \"\"\"Computes the Fisher vector on a set of descriptors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    xx: array_like, shape (N, D) or (D, )\n",
    "        The set of descriptors\n",
    "    gmm: instance of sklearn mixture.GMM object\n",
    "        Gauassian mixture model of the descriptors.\n",
    "    Returns\n",
    "    -------\n",
    "    fv: array_like, shape (K + 2 * D * K, )\n",
    "        Fisher vector (derivatives with respect to the mixing weights, means\n",
    "        and variances) of the given descriptors.\n",
    "    Reference\n",
    "    ---------\n",
    "    J. Krapac, J. Verbeek, F. Jurie.  Modeling Spatial Layout with Fisher\n",
    "    Vectors for Image Categorization.  In ICCV, 2011.\n",
    "    http://hal.inria.fr/docs/00/61/94/03/PDF/final.r1.pdf\n",
    "    \"\"\"\n",
    "    xx = np.atleast_2d(xx)\n",
    "    N = xx.shape[0]\n",
    "\n",
    "    # Compute posterior probabilities.\n",
    "    Q = gmm.compute_posteriors(xx)  # NxK\n",
    "    \n",
    "    Q = Q.asarray()\n",
    "\n",
    "    # Compute the sufficient statistics of descriptors.\n",
    "    Q_sum = np.sum(Q, 0)[:, np.newaxis] / N\n",
    "    Q_xx = np.dot(Q.T, xx) / N\n",
    "    Q_xx_2 = np.dot(Q.T, xx ** 2) / N\n",
    "\n",
    "    # Compute derivatives with respect to mixing weights, means and variances.\n",
    "    d_pi = Q_sum.squeeze() - gmm.get_weights()\n",
    "    d_mu = Q_xx - Q_sum * gmm.get_means()\n",
    "    d_sigma = (\n",
    "        - Q_xx_2\n",
    "        - Q_sum * gmm.get_means() ** 2\n",
    "        + Q_sum * gmm.get_covars()\n",
    "        + 2 * Q_xx * gmm.get_means())\n",
    "\n",
    "    # Merge derivatives into a vector.\n",
    "    return np.hstack((d_pi, d_mu.flatten(), d_sigma.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import ndimage, misc\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import HDFStore, DataFrame\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_dir = ('../dataset_h5/')\n",
    "\n",
    "h5f = h5py.File(os.path.join(dataset_dir,'images_224_delta_1.5.h5'),'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##EXAMPLE OF IMAGES WITH NO SIFT POINTS\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(h5f['data'][144].T, interpolation='nearest')\n",
    "plt.figure()\n",
    "plt.imshow(h5f['data'][406].T, interpolation='nearest')\n",
    "plt.figure()\n",
    "plt.imshow(h5f['data'][609].T, interpolation='nearest')\n",
    "plt.figure()\n",
    "plt.imshow(h5f['data'][732].T, interpolation='nearest')\n",
    "plt.figure()\n",
    "plt.imshow(h5f['data'][869].T, interpolation='nearest')\n",
    "plt.figure()\n",
    "plt.imshow(h5f['data'][923].T, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#image_features = process_images(h5f['data'])\n",
    "# sift.detectAndCompute(image_bgr, None)\n",
    "# cv2.xfeatures2d.SIFT_create().detectAndCompute(h5f['data'][0], None)\n",
    "#_, image_features = sift.detectAndCompute(image_bgr, None)\n",
    "\n",
    "#h5f['data'][144]\n",
    "# cv2.imshow('image',h5f['data'][144])\n",
    "filename = 'descriptors_images_224_delta_1.5.h5'\n",
    "if(os.path.isfile(os.path.join(dataset_dir,filename))):\n",
    "    descriptor_h5f = h5py.File(os.path.join(dataset_dir,filename),'r')\n",
    "    image_descriptors = descriptor_h5f['descriptors']\n",
    "else:\n",
    "    descriptor_h5f = h5py.File(os.path.join(dataset_dir,filename),'w')\n",
    "    image_descriptors = process_images(h5f['data'][:10000])\n",
    "    descriptor_h5f['descriptors'] = image_descriptors\n",
    "# image_descriptors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=64)# adjust yourself\n",
    "pca.fit(image_descriptors)\n",
    "image_descriptors_reduced = pca.transform(image_descriptors)\n",
    "# X_t_test = pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_descriptors_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "descriptor_h5f['descriptor_reduced'] = image_descriptors_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import ndimage, misc\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import HDFStore, DataFrame\n",
    "\n",
    "import h5py\n",
    "\n",
    "dataset_dir = ('../dataset_h5/')\n",
    "filename = 'descriptors_images_224_delta_1.5.h5'\n",
    "descriptor_h5f = h5py.File(os.path.join(dataset_dir,filename),'r')\n",
    "image_descriptors_reduced = descriptor_h5f['descriptor_reduced'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# full_dataset = h5f['data'][:]\n",
    "# full_dataset = full_dataset.reshape(full_dataset.shape[0], -1)\n",
    "# randomly_sampled = np.random.choice(full_dataset.shape[0], size=10000, replace=False)\n",
    "\n",
    "# X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "# X_test = h5f['data'][-10000: ]\n",
    "# X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "import ggmm.gpu as ggmm\n",
    "\n",
    "N, D = image_descriptors_reduced.shape\n",
    "K=128\n",
    "\n",
    "ggmm.init(2731155)\n",
    "gmm = ggmm.GMM(K,D)\n",
    "\n",
    "\n",
    "thresh = 1e-3 # convergence threshold\n",
    "n_iter = 20 # maximum number of EM iterations\n",
    "init_params = 'wmc' # initialize weights, means, and covariances\n",
    "\n",
    "# train GMM\n",
    "gmm.fit(image_descriptors_reduced, thresh, n_iter, init_params=init_params)\n",
    "\n",
    "# retrieve parameters from trained GMM\n",
    "weights = gmm.get_weights()\n",
    "means = gmm.get_means()\n",
    "covars = gmm.get_covars()\n",
    "\n",
    "# compute posteriors of data\n",
    "posteriors = gmm.compute_posteriors(image_descriptors_reduced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fv = fisher_vector(image_descriptors_reduced, gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16512,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import ndimage, misc\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import HDFStore, DataFrame\n",
    "\n",
    "import h5py\n",
    "\n",
    "dataset_dir = ('../dataset_h5')\n",
    "filename = 'descriptors_images_224_delta_1.5.h5'\n",
    "descriptor_h5f = h5py.File(os.path.join(dataset_dir,filename),'r')\n",
    "\n",
    "\n",
    "fv = descriptor_h5f['fisher_vector']\n",
    "\n",
    "fv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.79233216e-05,  -8.29181364e-06,   5.64052493e-05, ...,\n",
       "         1.52099688e-02,   2.96574679e-02,   1.00977729e-02])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fv[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(gmm, features):\n",
    "    X = features\n",
    "    Y = \n",
    "\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(X, Y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = train(gmm, fv)\n",
    "# rate = success_rate(classifier, fv)\n",
    "# print(\"Success rate is\", rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "descriptors = cv2.SIFT().detectAndCompute(full_dataset[0], None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
