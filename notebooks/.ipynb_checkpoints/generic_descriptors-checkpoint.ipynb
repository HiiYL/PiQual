{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import cv2\n",
    "\n",
    "index = 0\n",
    "def extract_image_features(image):\n",
    "    global index\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    image_bgr = cv2.cvtColor(image.T, cv2.COLOR_RGB2BGR)\n",
    "    _ , descriptors =  sift.detectAndCompute(image_bgr, None)\n",
    "    try:\n",
    "        descriptors.shape\n",
    "    except AttributeError:\n",
    "        print(index)\n",
    "        index = index + 1\n",
    "        return np.empty([0, 128])\n",
    "    index = index + 1\n",
    "    return descriptors\n",
    "\n",
    "def process_images(images):\n",
    "    return [extract_image_features(image) for image in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fisher_vector(xx, gmm):\n",
    "    \"\"\"Computes the Fisher vector on a set of descriptors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    xx: array_like, shape (N, D) or (D, )\n",
    "        The set of descriptors\n",
    "    gmm: instance of sklearn mixture.GMM object\n",
    "        Gauassian mixture model of the descriptors.\n",
    "    Returns\n",
    "    -------\n",
    "    fv: array_like, shape (K + 2 * D * K, )\n",
    "        Fisher vector (derivatives with respect to the mixing weights, means\n",
    "        and variances) of the given descriptors.\n",
    "    Reference\n",
    "    ---------\n",
    "    J. Krapac, J. Verbeek, F. Jurie.  Modeling Spatial Layout with Fisher\n",
    "    Vectors for Image Categorization.  In ICCV, 2011.\n",
    "    http://hal.inria.fr/docs/00/61/94/03/PDF/final.r1.pdf\n",
    "    \"\"\"\n",
    "    xx = np.atleast_2d(xx)\n",
    "    N = xx.shape[0]\n",
    "\n",
    "    # Compute posterior probabilities.\n",
    "    Q = gmm.compute_posteriors(xx)  # NxK\n",
    "    \n",
    "    Q = Q.asarray()\n",
    "\n",
    "    # Compute the sufficient statistics of descriptors.\n",
    "    Q_sum = np.sum(Q, 0)[:, np.newaxis] / N\n",
    "    Q_xx = np.dot(Q.T, xx) / N\n",
    "    Q_xx_2 = np.dot(Q.T, xx ** 2) / N\n",
    "\n",
    "    # Compute derivatives with respect to mixing weights, means and variances.\n",
    "    d_pi = Q_sum.squeeze() - gmm.get_weights()\n",
    "    d_mu = Q_xx - Q_sum * gmm.get_means()\n",
    "    d_sigma = (\n",
    "        - Q_xx_2\n",
    "        - Q_sum * gmm.get_means() ** 2\n",
    "        + Q_sum * gmm.get_covars()\n",
    "        + 2 * Q_xx * gmm.get_means())\n",
    "\n",
    "    # Merge derivatives into a vector.\n",
    "    return np.hstack((d_pi, d_mu.flatten(), d_sigma.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import ndimage, misc\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import HDFStore, DataFrame\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_dir = ('../dataset_h5/')\n",
    "\n",
    "h5f = h5py.File(os.path.join(dataset_dir,'images_224_delta_1.5.h5'),'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##EXAMPLE OF IMAGES WITH NO SIFT POINTS\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(h5f['data'][144].T, interpolation='nearest')\n",
    "plt.figure()\n",
    "plt.imshow(h5f['data'][406].T, interpolation='nearest')\n",
    "plt.figure()\n",
    "plt.imshow(h5f['data'][609].T, interpolation='nearest')\n",
    "plt.figure()\n",
    "plt.imshow(h5f['data'][732].T, interpolation='nearest')\n",
    "plt.figure()\n",
    "plt.imshow(h5f['data'][869].T, interpolation='nearest')\n",
    "plt.figure()\n",
    "plt.imshow(h5f['data'][923].T, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_descriptors = np.asarray(process_images(h5f['data'][:10000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'descriptors_images_224_delta_1.5.h5'\n",
    "if(os.path.isfile(os.path.join(dataset_dir,filename))):\n",
    "    descriptor_h5f = h5py.File(os.path.join(dataset_dir,filename),'r')\n",
    "    images_descriptors = descriptor_h5f['image_descriptors']\n",
    "else:\n",
    "    descriptor_h5f = h5py.File(os.path.join(dataset_dir,filename),'w')\n",
    "    image_descriptors = np.asarray(process_images(h5f['data'][:10000]))\n",
    "    descriptor_h5f['image_descriptors'] = image_descriptors\n",
    "# image_descriptors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save('image_descriptors.npy', image_descriptors)\n",
    "\n",
    "image_descriptors = np.load('image_descriptors.npy')\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=64)# adjust yourself\n",
    "pca.fit(np.concatenate(image_descriptors[:5000]))\n",
    "image_descriptors_reduced = np.asarray([ pca.transform(image) for image in image_descriptors if image.shape[0] > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_descriptors_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "descriptor_h5f['descriptor_reduced'] = image_descriptors_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import ndimage, misc\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import HDFStore, DataFrame\n",
    "\n",
    "import h5py\n",
    "\n",
    "dataset_dir = ('../dataset_h5/')\n",
    "filename = 'descriptors_images_224_delta_1.5.h5'\n",
    "descriptor_h5f = h5py.File(os.path.join(dataset_dir,filename),'r')\n",
    "image_descriptors_reduced = descriptor_h5f['descriptor_reduced'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# full_dataset = h5f['data'][:]\n",
    "# full_dataset = full_dataset.reshape(full_dataset.shape[0], -1)\n",
    "# randomly_sampled = np.random.choice(full_dataset.shape[0], size=10000, replace=False)\n",
    "\n",
    "# X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "# X_test = h5f['data'][-10000: ]\n",
    "# X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "import ggmm.gpu as ggmm\n",
    "import numpy as np\n",
    "\n",
    "image_descriptors_reduced = np.load(\"image_descriptors_reduced.npy\")\n",
    "concatenated_descriptors = np.concatenate(image_descriptors_reduced)\n",
    "N, D = concatenated_descriptors.shape\n",
    "K=128\n",
    "\n",
    "ggmm.init(2731155)\n",
    "gmm = ggmm.GMM(K,D)\n",
    "\n",
    "\n",
    "thresh = 1e-3 # convergence threshold\n",
    "n_iter = 20 # maximum number of EM iterations\n",
    "init_params = 'wmc' # initialize weights, means, and covariances\n",
    "\n",
    "# train GMM\n",
    "gmm.fit(concatenated_descriptors, thresh, n_iter, init_params=init_params)\n",
    "\n",
    "# # retrieve parameters from trained GMM\n",
    "# weights = gmm.get_weights()\n",
    "# means = gmm.get_means()\n",
    "# covars = gmm.get_covars()\n",
    "\n",
    "# # compute posteriors of data\n",
    "# posteriors = gmm.compute_posteriors(image_descriptors_reduced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fv = fisher_vector(image_descriptors_reduced, gmm)\n",
    "\n",
    "fv = [ fisher_vector(image,gmm) for image in image_descriptors_reduced]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import ndimage, misc\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import HDFStore, DataFrame\n",
    "\n",
    "import h5py\n",
    "\n",
    "dataset_dir = ('../dataset_h5')\n",
    "filename = 'descriptors_images_224_delta_1.5.h5'\n",
    "descriptor_h5f = h5py.File(os.path.join(dataset_dir,filename),'r')\n",
    "\n",
    "\n",
    "fv = descriptor_h5f['feature_vector']\n",
    "\n",
    "fv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.79233216e-05,  -8.29181364e-06,   5.64052493e-05, ...,\n",
       "         1.52099688e-02,   2.96574679e-02,   1.00977729e-02])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fv[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "def train(gmm, features, labels):\n",
    "    X = features\n",
    "    Y = labels\n",
    "\n",
    "    clf = svm.LinearSVC()\n",
    "    clf.fit(X, Y)\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import HDFStore, DataFrame\n",
    "import numpy as np\n",
    "# image_with_no_features = []\n",
    "# index=0\n",
    "# for image in image_descriptors:\n",
    "#     if image.shape[0] <= 0:\n",
    "#         image_with_no_features.append(index)\n",
    "#     index = index + 1\n",
    "image_with_no_features =  np.load(\"image_with_no_features.npy\")\n",
    "\n",
    "\n",
    "\n",
    "image_no_feat_cropped = [i for i in image_with_no_features if i <= 1000]\n",
    "\n",
    "store = HDFStore('../dataset_h5/labels.h5')\n",
    "ava_table = store['labels']\n",
    "temp = ava_table.head(1000)\n",
    "temp = temp.drop(temp.iloc[image_no_feat_cropped].index.values)\n",
    "\n",
    "\n",
    "\n",
    "labels = temp.good\n",
    "labels_test = temp2.good\n",
    "\n",
    "fv = np.load(\"fisher_vector.npy\")\n",
    "gmm = np.load(\"\")\n",
    "\n",
    "classifier = train(gmm, fv[:993,:],labels)\n",
    "rate = success_rate(classifier, fv[:993,:])\n",
    "print(\"Success rate is\", rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "temp2 = ava_table.head(10000)\n",
    "temp2 = temp2.drop(temp2.iloc[image_with_no_features].index.values)\n",
    "\n",
    "classifier = train(gmm, fv,labels_test)\n",
    "labels_test = temp2.good\n",
    "\n",
    "accuracy_score(labels, classifier.predict(fv[:993,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "descriptors = cv2.SIFT().detectAndCompute(full_dataset[0], None)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
